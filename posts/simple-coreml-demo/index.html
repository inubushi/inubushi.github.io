<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Simple iOS Coreml Demo: Image Classification | The AIPhone Blog</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.70.0" />
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    
    
      <link href="/dist/css/app.1cb140d8ba31d5b2f1114537dd04802a.css" rel="stylesheet">
    

    

    
      
    

    
    
    <meta property="og:title" content="Simple iOS Coreml Demo: Image Classification" />
<meta property="og:description" content="Image classification is a very basic task, in the point of view of humans. We just have to look at a photo that is supposed to belong to one of n categories, and tell which category it belongs to.
In AI terms, the photo is called an image; the categories are called classes. The AI (in our case a Deep Neural Network) processes the input image, and decides how likely it is to belong to each class." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://inubushi.github.io/posts/simple-coreml-demo/" />
<meta property="article:published_time" content="2020-05-19T03:28:58+09:00" />
<meta property="article:modified_time" content="2020-05-19T03:28:58+09:00" />
<meta itemprop="name" content="Simple iOS Coreml Demo: Image Classification">
<meta itemprop="description" content="Image classification is a very basic task, in the point of view of humans. We just have to look at a photo that is supposed to belong to one of n categories, and tell which category it belongs to.
In AI terms, the photo is called an image; the categories are called classes. The AI (in our case a Deep Neural Network) processes the input image, and decides how likely it is to belong to each class.">
<meta itemprop="datePublished" content="2020-05-19T03:28:58&#43;09:00" />
<meta itemprop="dateModified" content="2020-05-19T03:28:58&#43;09:00" />
<meta itemprop="wordCount" content="742">



<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Simple iOS Coreml Demo: Image Classification"/>
<meta name="twitter:description" content="Image classification is a very basic task, in the point of view of humans. We just have to look at a photo that is supposed to belong to one of n categories, and tell which category it belongs to.
In AI terms, the photo is called an image; the categories are called classes. The AI (in our case a Deep Neural Network) processes the input image, and decides how likely it is to belong to each class."/>

  </head>

  <body class="ma0 avenir near-black bg-near-white">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="https://inubushi.github.io/" class="f3 fw2 hover-white no-underline white-90 dib">
      The AIPhone Blog
    </a>
    <div class="flex-l items-center">
      

      
      














    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked">
          
        POSTS
      </aside>
      




  <div id="sharing" class="mt3">

    
    
    <a href="https://twitter.com/share?url=https://inubushi.github.io/posts/simple-coreml-demo/&amp;text=Simple%20iOS%20Coreml%20Demo:%20Image%20Classification" class="twitter no-underline" aria-label="share on Twitter">
      <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167,22.283c-2.619,0.953-4.274,3.411-4.086,6.101  l0.063,1.038l-1.048-0.127c-3.813-0.487-7.145-2.139-9.974-4.915l-1.383-1.377l-0.356,1.017c-0.754,2.267-0.272,4.661,1.299,6.271  c0.838,0.89,0.649,1.017-0.796,0.487c-0.503-0.169-0.943-0.296-0.985-0.233c-0.146,0.149,0.356,2.076,0.754,2.839  c0.545,1.06,1.655,2.097,2.871,2.712l1.027,0.487l-1.215,0.021c-1.173,0-1.215,0.021-1.089,0.467  c0.419,1.377,2.074,2.839,3.918,3.475l1.299,0.444l-1.131,0.678c-1.676,0.976-3.646,1.526-5.616,1.568  C19.775,43.256,19,43.341,19,43.405c0,0.211,2.557,1.397,4.044,1.864c4.463,1.377,9.765,0.783,13.746-1.568  c2.829-1.673,5.657-5,6.978-8.221c0.713-1.716,1.425-4.851,1.425-6.354c0-0.975,0.063-1.102,1.236-2.267  c0.692-0.678,1.341-1.419,1.467-1.631c0.21-0.403,0.188-0.403-0.88-0.043c-1.781,0.636-2.033,0.551-1.152-0.402  c0.649-0.678,1.425-1.907,1.425-2.267c0-0.063-0.314,0.042-0.671,0.233c-0.377,0.212-1.215,0.53-1.844,0.72l-1.131,0.361l-1.027-0.7  c-0.566-0.381-1.361-0.805-1.781-0.932C39.766,21.902,38.131,21.944,37.167,22.283z M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4  s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/></svg>

    </a>

    
    <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://inubushi.github.io/posts/simple-coreml-demo/&amp;title=Simple%20iOS%20Coreml%20Demo:%20Image%20Classification" class="linkedin no-underline" aria-label="share on LinkedIn">
      <svg  height="32px"  style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>

    </a>
  </div>


      <h1 class="f1 athelas mt3 mb1">Simple iOS Coreml Demo: Image Classification</h1>
      
      
      <time class="f6 mv4 dib tracked" datetime="2020-05-19T03:28:58&#43;09:00">May 19, 2020</time>

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l"><p>Image classification is a very basic task,  in the point of view of humans. We
just have to look at a photo that is supposed to belong to one of <em>n</em> categories,
and tell which category it belongs to.</p>
<p>In AI terms, the photo is called an <em>image</em>; the categories are called <em>classes</em>.
The AI (in our case a <em>Deep Neural Network</em>) processes the input image, and
decides how likely it is to belong to each class. As output, it provides the ID
of the class for which the likelyhood is maximum.</p>
<p>This demo app shows how to use CoreML with a deep neural network that someone
has already created (we will get to creating our own AIs, in future posts). The
network that we have selected is called &ldquo;MobileNetV2&rdquo;. This was originally
created as part of an academic research project, but Apple has made a CoreML
version for you. Here is what it can do:</p>
<ol>
<li>Accept a color photo</li>
<li>Classify it to one of the 1000 classes listed
<a href="https://gist.githubusercontent.com/yrevar/942d3a0ac09ec9e5eb3a/raw/238f720ff059c1f82f368259d1ca4ffa5dd8f9f5/imagenet1000_clsidx_to_labels.txt">here</a>.</li>
</ol>
<p>Admittedly, there is no practical application for classifying a photo into one
of these classes. But remember, we are just trying to learn the basics.</p>
<p>For those who want to get the code quickly, it is available
<a href="https://github.com/inubushi/ios-coreml-simple">here</a>. I will explain the
important parts of the app below.</p>
<ol>
<li>Create a single view iOS Swift app.</li>
</ol>
<p>The first thing you need to do is adding CoreML framework to the ViewController
class (ViewController.swift, line 11).</p>
<pre><code>// we need to import CoreML framework
import CoreML
</code></pre><ol start="2">
<li>
<p>Download the CoreML model called &ldquo;MobileNetV2&rdquo; from
<a href="https://developer.apple.com/machine-learning/models/">here</a>,
and add it to the project.</p>
</li>
<li>
<p>Declare a class variable for MobileNetV2. We can initialize it immediately
since we are not going to change it within the app lifecycle
(ViewController.swift, line 20).</p>
</li>
</ol>
<pre><code>// using the MobileNet V2 model, because it is fast
let model = MobileNetV2()
</code></pre><ol start="4">
<li>The app includes code for using the camera to take a photo, and also to pick
a photo from a few samples. But the real work on image classification is done
in the function <code>classifyImage()</code> (ViewController.swift, line 90)</li>
</ol>
<pre><code>//MARK: Classification
func classifyImage(img:UIImage) {
    // conversion to the correct input size,
    // and get the image data to a pixel buffer
    UIGraphicsBeginImageContextWithOptions(CGSize(width: 224, height: 224), true, 2.0)
    img.draw(in: CGRect(x: 0, y: 0, width: 224, height: 224))
    let newImage = UIGraphicsGetImageFromCurrentImageContext()!
    UIGraphicsEndImageContext()

    let attrs = [kCVPixelBufferCGImageCompatibilityKey: kCFBooleanTrue, kCVPixelBufferCGBitmapContextCompatibilityKey: kCFBooleanTrue] as CFDictionary
    var pixelBuffer : CVPixelBuffer?
    let status = CVPixelBufferCreate(kCFAllocatorDefault, Int(newImage.size.width), Int(newImage.size.height), kCVPixelFormatType_32ARGB, attrs, &amp;pixelBuffer)
    guard (status == kCVReturnSuccess) else {
        return
    }

    CVPixelBufferLockBaseAddress(pixelBuffer!, CVPixelBufferLockFlags(rawValue: 0))
    let pixelData = CVPixelBufferGetBaseAddress(pixelBuffer!)

    let rgbColorSpace = CGColorSpaceCreateDeviceRGB()
    let context = CGContext(data: pixelData, width: Int(newImage.size.width), height: Int(newImage.size.height), bitsPerComponent: 8, bytesPerRow: CVPixelBufferGetBytesPerRow(pixelBuffer!), space: rgbColorSpace, bitmapInfo: CGImageAlphaInfo.noneSkipFirst.rawValue)

    context?.translateBy(x: 0, y: newImage.size.height)
    context?.scaleBy(x: 1.0, y: -1.0)

    UIGraphicsPushContext(context!)
    newImage.draw(in: CGRect(x: 0, y: 0, width: newImage.size.width, height: newImage.size.height))
    UIGraphicsPopContext()
    CVPixelBufferUnlockBaseAddress(pixelBuffer!, CVPixelBufferLockFlags(rawValue: 0))

    // forward pass with time measurement
    let start = DispatchTime.now()
    guard let prediction = try? model.prediction(image: pixelBuffer!) else {
        return
    }
    let end = DispatchTime.now()

    let nanoTime = end.uptimeNanoseconds - start.uptimeNanoseconds // processing time in nano seconds (UInt64)
    let timeInterval = Double(nanoTime) / 1_000_000 // convert to milliseconds, for ease of reading

    var resultString: String = &quot;AI thinks this is a \(prediction.classLabel).&quot;
    resultString += String(format:&quot;\n(%f milliseconds)&quot;, timeInterval)

    // update results on main thread
    DispatchQueue.main.async {
        self.activityIndicator.stopAnimating()
        self.labelInfo.text = resultString
    }
}
</code></pre><p>Most deep neural networks require input images to be at a pre-determind size.
For MobileNetV2, the size is 224x224 pixels. We first resize the photo to this
size, and get the data to <code>pixelBuffer</code>.</p>
<p>The pixel buffer is then passed as input to MobileNetV2, for prediction.</p>
<pre><code>guard let prediction = try? model.prediction(image: pixelBuffer!) else {
    return
}
</code></pre><p>The variable <code>prediction.classLabel</code> contains a string describing the
category that the photo belongs to.</p>
<p>You will note that I have also added some code to calculate the time taken by
the model to predict the class. Most of the good models are large, and take a
considerable amount of time for prediction. On a new iPhone like iPhone 11 Pro,
MobileNetV2 takes less than 10 milliseconds for prediction. This is because the
<em>Bionic</em> processor in new iPhones has been specifically designed to support
deep neural networking functions.</p>
<p>So, you now have a very simple iPhone app that can load a trained deep neural
network and use it to classify images. In coming posts, I will build up on this
to carry out other tasks like detection and segmentation, and also explain how
to train your own neural networks.</p>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://inubushi.github.io/" >
    &copy;  The AIPhone Blog 2020 
  </a>
    <div>













</div>
  </div>
</footer>

    

  <script src="/dist/js/app.3fc0f988d21662902933.js"></script>


  </body>
</html>
